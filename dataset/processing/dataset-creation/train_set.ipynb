{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_DIR = 'csv/big_csv/'\n",
    "\n",
    "PROPORTION = 0.8\n",
    "RANDOM_STATE = 42\n",
    "MEAN_WEIGHT = 1\n",
    "OVERRIDE_TOP = 0\n",
    "\n",
    "# Commands used to only show the progress were commented out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "orig_dataset = pd.read_csv(CSV_DIR + 'filtered_datetime.csv')\n",
    "#orig_dataset = pd.read_csv(CSV_DIR + 'train_data.csv')\n",
    "# orig_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "orig_dataset = orig_dataset.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "# orig_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# shows the dataframe summarized in a table\n",
    "def get_table(df, print_table = False):\n",
    "    counts_df = df.groupby(['species', 'park']).size().unstack(fill_value=0)\n",
    "    counts_df['Total'] = counts_df.sum(axis=1)\n",
    "    counts_df.loc['Total'] = counts_df.sum()\n",
    "    if print_table: print(counts_df)\n",
    "    return counts_df\n",
    "\n",
    "\n",
    "# get the mean number of images per species\n",
    "mean_images_per_species = lambda x: x[x['species'] != 'emp'].groupby('species')['path'].count().mean()\n",
    "# round mean to next thousand and multiply by weight\n",
    "rounded_mean = lambda x: ((((mean_images_per_species(x)) + 999) // 1000) * 1000) * MEAN_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# get_table(orig_dataset, print_table=True)\n",
    "# print(rounded_mean(orig_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each species and park take a sample of PROPORTION% of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# generate train dataset\n",
    "train_dataset = orig_dataset.groupby(['species', 'park']).apply(lambda x: x.sample(frac=PROPORTION, random_state=RANDOM_STATE)).reset_index(drop=True)\n",
    "# test dataset are the discarded rows, it will be 1-PROPORTION of the original dataset\n",
    "test_dataset = orig_dataset[~orig_dataset.index.isin(train_dataset.index)]\n",
    "# print(rounded_mean(train_dataset), rounded_mean(test_dataset))\n",
    "\n",
    "# DISPLAY THE DATAFRAMES\n",
    "# df1 = get_table(orig_dataset)\n",
    "# df2 = get_table(train_dataset)\n",
    "# df1 = pd.concat([df1, df2], axis=1)\n",
    "# df2 = get_table(test_dataset)\n",
    "# concatenated_df = pd.concat([df1, df2], axis=1)\n",
    "# display(concatenated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_even_images will take only a maximum of top images per species (top is the rounded mean). Will distribute the images such that between parks it will try 50/50 if not possible it will take more than the other to compensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_even_images(dataset, top):\n",
    "    top = int(top)\n",
    "    selected_images = pd.DataFrame()\n",
    "    for species in dataset['species'].unique():\n",
    "        images_of_species = dataset[dataset['species'] == species]\n",
    "        total_images_for_species = images_of_species.shape[0]\n",
    "\n",
    "        # Calculate the total number of images available in all parks for this species\n",
    "        total_images_in_parks = images_of_species.groupby('park').size().to_dict()\n",
    "\n",
    "        # Calculate the number of images to take from each park based on their relative sizes\n",
    "        images_per_park = {park: min(int(top/2), total_images_in_parks[park]) for park in total_images_in_parks}\n",
    "        images_per_park = dict(sorted(images_per_park.items(), key=lambda item: item[1], reverse=False))\n",
    "        \n",
    "        next_is_taking = int(top/2)\n",
    "        if len(images_per_park) == 1:\n",
    "            images_taken = images_of_species.head(top)\n",
    "            selected_images = pd.concat([selected_images, images_taken])\n",
    "            continue\n",
    "        for park in images_per_park:\n",
    "            images_taken = images_of_species[images_of_species['park'] == park].head(next_is_taking)\n",
    "            next_is_taking = top - images_taken.shape[0]\n",
    "            selected_images = pd.concat([selected_images, images_taken])\n",
    "            \n",
    "    return selected_images.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# If override top is set, it will use the value as the top number of images to take instead of the rounded mean\n",
    "train_dataset = get_even_images(train_dataset, rounded_mean(train_dataset) if OVERRIDE_TOP == 0 else OVERRIDE_TOP)\n",
    "test_dataset = get_even_images(test_dataset, rounded_mean(test_dataset) if OVERRIDE_TOP == 0 else OVERRIDE_TOP)\n",
    "\n",
    "# df1 = get_table(train_dataset)\n",
    "# df2 = get_table(test_dataset)\n",
    "# concatenated_df = pd.concat([df1, df2], axis=1)\n",
    "# display(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# save dataframes\n",
    "train_dataset.to_csv(CSV_DIR + 'train_data.csv', index=False)\n",
    "test_dataset.to_csv(CSV_DIR + 'val_data.csv', index=False) # Change filename to val_data and rerun the last two cells for val split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
