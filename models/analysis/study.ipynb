{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6af8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import *\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "LABELS = ['emp', 'bos', 'caae', 'caca', 'can', 'capi', 'cer', 'dam', 'equ', 'fel', 'fsi', 'gen', 'her', 'lep', 'lut', 'lyn', 'mafo', 'mel', 'mus', 'ory', 'ovar', 'ovor', 'rara', 'sus', 'vul']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf02798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a model's values and do sklearn metrics.\n",
    "def read_values(file_path):\n",
    "    true_values = []\n",
    "    pred_values = []\n",
    "    iou_values = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('True vals:'):\n",
    "                true_values = ast.literal_eval(line.split(':')[1].strip())\n",
    "            elif line.startswith('Pred vals:'):\n",
    "                pred_values = ast.literal_eval(line.split(':')[1].strip())\n",
    "            elif line.startswith('IoU vals:'):\n",
    "                iou_values = ast.literal_eval(line.split(':')[1].strip())\n",
    "\n",
    "    return true_values, pred_values, iou_values\n",
    "\n",
    "def calculate_mAP(true, pred, iou, iou_threshold=0.5):\n",
    "    true_positives = defaultdict(int)\n",
    "    false_positives = defaultdict(int)\n",
    "    false_negatives = defaultdict(int)\n",
    "    \n",
    "    idx = 0\n",
    "    for t, p in zip(true, pred):\n",
    "        if t != -1 and p != -1:\n",
    "            i = iou[idx]\n",
    "            if i >= iou_threshold:\n",
    "                true_positives[p] += 1\n",
    "            else:\n",
    "                false_positives[p] += 1\n",
    "                false_negatives[t] += 1\n",
    "            idx += 1\n",
    "\n",
    "    APs = {}\n",
    "    for clas in set(true + pred):\n",
    "        tp = true_positives[clas]\n",
    "        fp = false_positives[clas]\n",
    "    \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        APs[clas] = precision\n",
    "\n",
    "    mAP = np.mean(list(APs.values()))\n",
    "    return mAP, APs\n",
    "\n",
    "def parse_results(model):\n",
    "    true, pred, iou = read_values(f\"{model}_values.txt\")\n",
    "    results = {\n",
    "        \"precision\": precision_score(true, pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(true, pred, average='weighted', zero_division=0),\n",
    "        \"f1\": f1_score(true, pred, average='weighted', zero_division=0),\n",
    "        \"accuracy\": accuracy_score(true, pred),\n",
    "        \"iou\": sum(iou) / len(iou) if len(iou) > 0 else 0,\n",
    "        \"matrix\": confusion_matrix(true, pred),\n",
    "        \"report\": classification_report(true, pred, target_names=LABELS, zero_division=0, output_dict=True),\n",
    "        \"mAP\": defaultdict(dict),\n",
    "    }\n",
    "\n",
    "    for i in range(50, 100, 5):\n",
    "        f = i/100\n",
    "        mAP, APs = calculate_mAP(true, pred, iou, iou_threshold=f)\n",
    "        results[\"mAP\"][f] = (float(mAP), APs)\n",
    "    return results\n",
    "\n",
    "models = {\n",
    "    \"YOLO\": parse_results(\"yolo\"),\n",
    "    \"FasterRCNN\": parse_results(\"fasterrcnn\"),\n",
    "    \"Megadetector\": parse_results(\"megadetector\")\n",
    "}\n",
    "\n",
    "COLORS = {\n",
    "    \"YOLO\": \"#539045\",\n",
    "    \"FasterRCNN\": \"#44739d\",\n",
    "    \"Megadetector\": \"#d48640\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"model\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "    \"iou\": [],\n",
    "    \"accuracy\": []\n",
    "})\n",
    "\n",
    "for model in models.keys():\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        \"model\": [model],\n",
    "        \"precision\": [models[model][\"precision\"]],\n",
    "        \"recall\": [models[model][\"recall\"]],\n",
    "        \"f1\": [models[model][\"f1\"]],\n",
    "        \"iou\": [models[model][\"iou\"]],\n",
    "        \"accuracy\": [models[model][\"accuracy\"]]\n",
    "    })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_plot(table, model):\n",
    "    metrics = [\"precision\", \"recall\", \"f1\", \"iou\", \"accuracy\"]\n",
    "    names = [\"Precision\", \"Recall\", \"F1 Score\", \"IoU\", \"Accuracy\"]\n",
    "    \n",
    "    table[\"metric\"] = pd.Categorical(table[\"metric\"], categories=metrics, ordered=True)\n",
    "    table = table.sort_values(by=\"metric\")\n",
    "    table[\"metric\"] = table[\"metric\"].map(dict(zip(metrics, names)))\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    cmap = (sns.light_palette(COLORS[model], as_cmap=False, n_colors=4)[-2:]*3)[1:]\n",
    "    ax = sns.barplot(x=\"value\", y=\"metric\", data=table, hue=\"metric\", dodge=False, palette=cmap)\n",
    "    \n",
    "    for bar in ax.patches:\n",
    "        plt.text(\n",
    "            bar.get_width() + 0.005,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{bar.get_width():.3f}\",\n",
    "            va=\"center\",\n",
    "            ha=\"left\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "    \n",
    "    xticks = np.linspace(0.5, 1, num=11)\n",
    "    plt.xticks(ticks=xticks, labels=[f\"{tick:.2f}\" if i % 2 == 0 else \"\" for i, tick in enumerate(xticks)])\n",
    "\n",
    "    for tick in ax.get_xticks():\n",
    "        ax.axvline(x=tick, color='lightgray', linestyle='--', linewidth=0.5, alpha=0.8, zorder=0)\n",
    "    \n",
    "    plt.xlim(0.5, 1)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.savefig(f\"images/{model.lower()}_metrics.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "model_tables = {\n",
    "    model: results[results[\"model\"] == model]\n",
    "    .melt(id_vars=[\"model\"], var_name=\"metric\", value_name=\"value\")\n",
    "    .drop(columns=\"model\")\n",
    "    for model in results[\"model\"].unique()\n",
    "}\n",
    "\n",
    "for model, table in model_tables.items():\n",
    "    get_model_plot(table, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f29c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to compare all model's metrics\n",
    "def plot_all_metrics(results, colors):\n",
    "    metrics = [\"precision\", \"accuracy\", \"recall\", \"f1\", \"iou\"]\n",
    "    names = [\"Precision\", \"Accuracy\", \"Recall\", \"F1 Score\", \"IoU\"]\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for i, model in enumerate(results[\"model\"]):\n",
    "        values = results.loc[results[\"model\"] == model, metrics].values.flatten()\n",
    "        ax.bar(x + i * width, values, width, label=model, color=colors[model])\n",
    "\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    \n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([i * 0.05 for i in range(21)])\n",
    "    ax.set_yticklabels([f\"{i * 0.05:.2f}\" if i % 2 == 0 else \"\" for i in range(21)])\n",
    "    for tick in plt.yticks()[0]:\n",
    "        if tick*100 % 2: plt.axhline(y=tick, color='lightgray', linestyle='--', linewidth=0.5, alpha=0.5, zorder=0)\n",
    "        else: plt.axhline(y=tick, color='lightgray', linestyle='--', linewidth=0.5, alpha=0.8, zorder=0)\n",
    "\n",
    "    ax.legend(title=\"Models\", loc=\"upper center\", bbox_to_anchor=(0.5, -0.1), ncol=len(results[\"model\"].unique()))\n",
    "\n",
    "    for bar in ax.patches:\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{bar.get_height():.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/metrics_across_models.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_all_metrics(results, COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix\n",
    "def plot_confusion_matrix(matrix, model):\n",
    "    matrix = round(matrix.div(matrix.sum(axis=1), axis=0).iloc[::-1] * 100, 1)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    cmap = sns.blend_palette([\"white\", COLORS[model]], as_cmap=True)\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', vmin=0, vmax=100, cmap=cmap)\n",
    "    step_values = [i for i in range(len(LABELS))]\n",
    "    plt.xticks(ticks=[i + 0.5 for i in step_values], labels=LABELS, rotation=45)\n",
    "    plt.yticks(ticks=[i + 0.5 for i in step_values[::-1]], labels=LABELS, rotation=45)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"images/matrix_{model.lower()}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "for model in models.keys():\n",
    "    matrix = pd.DataFrame(models[model][\"matrix\"])\n",
    "    plot_confusion_matrix(matrix, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f809a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of mAP vs IoU Threshold curves of all models\n",
    "def plot_all_mAP_vs_iou(models, colors):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for model, data in models.items():\n",
    "        mAP = data[\"mAP\"]\n",
    "        x = list(mAP.keys())\n",
    "        y = [mAP[i][0] for i in x]\n",
    "        plt.plot(x, y, marker='o', label=model, color=colors[model])\n",
    "    \n",
    "    plt.xticks(ticks=[i * 0.05 for i in range(21)], labels=[f\"{i * 0.05:.2f}\" for i in range(21)])\n",
    "    plt.xlim(0.5, 1)\n",
    "    plt.yticks(ticks=[i * 0.05 for i in range(21)], labels=[f\"{i * 0.05:.2f}\" for i in range(21)])\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.xlabel(\"IoU Threshold\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.legend(title=\"Models\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/map_across_models.png', dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "plot_all_mAP_vs_iou(models, COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of the f1 score for each species\n",
    "def plot_f1_scores(report, model):\n",
    "    f1_scores = [report[label]['f1-score'] for label in LABELS]\n",
    "    labels = LABELS\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cmap = (sns.light_palette(COLORS[model], as_cmap=False, n_colors=4)[-2:]*13)[1:]\n",
    "    sns.barplot(y=f1_scores, x=labels, hue=labels, palette=cmap, dodge=False)\n",
    "    plt.yticks(ticks=[i * 0.1 for i in range(11)])\n",
    "    for tick in plt.yticks()[0]:\n",
    "        plt.axhline(y=tick, color='lightgray', linestyle='--', linewidth=0.5, alpha=0.8, zorder=0)\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.xlabel(\"Species\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"images/f1_{model.lower()}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "for model in models.keys():\n",
    "    plot_f1_scores(models[model]['report'], model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
